{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MNIST Dataset from Scratch\n\nattempting to build a neural network to classify handwritten digits from scratch (just numpy + math). \n\nhere's the video that inspiried this: https://www.youtube.com/watch?v=w8yWXqWQYmU\n\ni'll do my best to actually think through everything, and not just copy the video (we'll see about that lol). i want to fully understand the math behind the neural network, which will help me grasp the fundamentals of machine/deep learning. \n\ni lied btw. ","metadata":{}},{"cell_type":"code","source":"# importing libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# these are the only libraries/frameworks that i'll use (no tensorflow or pytorch)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:36.031242Z","iopub.execute_input":"2025-01-04T07:39:36.031648Z","iopub.status.idle":"2025-01-04T07:39:36.036910Z","shell.execute_reply.started":"2025-01-04T07:39:36.031622Z","shell.execute_reply":"2025-01-04T07:39:36.035559Z"}},"outputs":[],"execution_count":213},{"cell_type":"markdown","source":"# Math \n\nhere i want to write out + explain the math being used to build the neural network. doing this so i actually understand the underlying operations of the neural network, and so that i know what i'm doing (i don't 😘). \n\ni'll be using the same architecture as the video. the input layer will have 784 (28x28) neurons, each corresponding to a pixel of each training example. the following hidden and output layers each have 10 neurons, with the output layer being a vector of probabilities, enabling the classification of the numbers in the dataset. below, i'll try to map out each step. \n\n## Forward Propagation\n\n### Input Layer\n\nthe input layer looks like this:\n\n$$A^{(0)} = X (784 * m)$$\n\nwhere m is the number of training examples. \n\nX represents all of the features (the pixels of each training example). \n\n### Hidden Layer\n\nthe unactivated hidden layer, composed of 10 neurons, is represented as this:\n\n$$Z^{(1)} = W^{(1)} \\cdot A^{(0)} + b^{(1)}$$\n\nthe dot in the equation represents the dot product of the weights (W1) and the input layer (A0). the bias is later added to the dot product (b1)\n\n### Activation Function (ReLU)\n \nwithout the activation function, the neural network ends up being just a fancy linear equation. the activation function introduces nonlinearity into the model, which is crucial for this use case (classification). \n\nin this model, i'll be using the ReLU function, like the video:\n\n$$A^{(1)} = g(Z^{(1)}) = ReLU(Z^{(1)})$$\n\nwhere:\n\n\\begin{align}\n    ReLU = \\begin{cases}\n            x, \\text{ if x > 0}\\\\\n            0, \\text{ if x ≤ 0}\n           \\end{cases}\n\\end{align}\n\n\n\nthe unactivated layer is passed onto the activation function, forming the hidden layer. \n\n### Output Layer\n\nthe unactivated final output layer, like the hidden layer, has 10 neurons, each corresponding to a digit. \n\n$$Z^{(2)} = W^{(2)} \\cdot A^{(1)} + b^{(2)}$$\n\n### Activation Function (Softmax)\n\nthe softmax activation function is then applied to the final layer to create a vector of probabilities, which is used for classification. \n\nthe function can be represented as this:\n\n$$A^{(2)} = softmax(Z^{(2)})$$\n\nwhere:\n\n$$softmax(Z^{(2)}) = \\frac {e^{z_i}} {\\sum\\limits_{j=1}^K e^{z_j}}$$\n\neach value in the resulting vector will be between 0 and 1.\n\nyay, we did it!\n\nexcept we didn't, because we still have **backpropagation**. yay...\n\n## Backpropagation \n\nfrom forward propagation, we obtained a vector of propabilities. using this, we can perform backpropagation, seeing how much the prediction deviated from the actual label of the training example and how much each of the previous weights contributed to the error. \n\nessentially, we're doing reverse forward propagation (in a way). \n\n### Output Layer\n\nwe first measure the error of the output layer by seeing the difference between the prediction and the label:\n\n$$dZ^{(2)} = A^{(2)} - Y$$\n\nfor the label (Y), we will perform one-hot encoding to convert the categories into a binary format (0 and 1) that the predictions can be compared to. \n\nwith this, we can now find the derivatives of the loss functions with respect to the weights and bias in the output layer:\n\n$$dW^{(2)} = \\frac {1} {m} dZ^{(2)} A^{(1)T}$$\n$$db^{(2)} = \\frac {1} {m} \\sum dZ^{(2)}$$\n\n### Hidden Layer\n\nwe perform similar calcuations for this layer: \n\n$$dZ^{(1)} = W^{(2)T}dZ^{(2)} * g'(Z^{(2)})$$ \n\nthe g' is the derivative of the activation function ReLU. \n\n$$dW^{(1)} = \\frac {1} {m} dZ^{(1)} X^{T}$$\n$$db^{(1)} = \\frac {1} {m} \\sum dZ^{(1)}$$\n\n### Adjustments\n\nnow after ALL of this, we adjust the weights and bias accordingly. \n\n$$w^{(1)} = w^{(1)} - αdW^{(1)}$$\n$$b^{(1)} = b^{(1)} - αdb^{(1)}$$\n$$w^{(2)} = w^{(2)} - αdW^{(2)}$$\n$$b^{(2)} = b^{(2)} - αdb^{(2)}$$\n\nwhere α is the learning rate. \n\nand then we do this over and over again until something cool happens. ","metadata":{}},{"cell_type":"markdown","source":"# Loading the Dataset\n\nok first, i need to actually load + format the data in a way that is compatible for the network. ","metadata":{}},{"cell_type":"code","source":"# loading the data\n\ndataframe = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\") # since the data is a csv file, i can load it in with pandas.read_csv() function.\ndataframe.head() # printing the first 5 training examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:36.046408Z","iopub.execute_input":"2025-01-04T07:39:36.046738Z","iopub.status.idle":"2025-01-04T07:39:38.294175Z","shell.execute_reply.started":"2025-01-04T07:39:36.046711Z","shell.execute_reply":"2025-01-04T07:39:38.293152Z"}},"outputs":[{"execution_count":214,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":214},{"cell_type":"code","source":"# now i need to separate the features and targets for classification. \n\n# dataframe to numpy array\ndata = np.array(dataframe)\nnp.random.shuffle(data) # shuffling the data\n\n# i'll also get the shape of the data for variables. \n\nm, n = data.shape\n\n# separating the features (X) and targets (y) \ndata_dev = data[0:1000].T\nX = data_dev[1:n]\nY = data[0]\n\ndata_train = data[1000:4000].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train / 255.\n\n# the labels happen to be the first column, rather than the last. \n\nX, Y # testing, and it worked!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:38.295546Z","iopub.execute_input":"2025-01-04T07:39:38.295996Z","iopub.status.idle":"2025-01-04T07:39:38.990479Z","shell.execute_reply.started":"2025-01-04T07:39:38.295957Z","shell.execute_reply":"2025-01-04T07:39:38.989435Z"}},"outputs":[{"execution_count":215,"output_type":"execute_result","data":{"text/plain":"(array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]),\n array([  2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,  33,  86, 255, 254, 184,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0, 106, 253, 253, 253, 226,\n         65,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,  68, 244, 253, 253,\n        253, 253, 243,  24,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 202, 253,\n        253, 204, 205, 253, 253,  25,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n        167, 209,  82,  22, 167, 253, 253,  56,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0, 167, 253, 253, 115,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0, 167, 253, 237,  23,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 167, 253, 104,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  65, 229,\n        253, 104,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   4,\n        145, 253, 228,  68,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0, 125, 253, 253, 182,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0, 184, 253, 253,  12,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,  63, 225, 253, 154,   3,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,  61, 238, 253, 234,  69,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0, 202, 253, 248,  80,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0, 103, 244, 253, 244,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0, 100, 181, 253, 253,\n        248, 115, 115, 115, 115,  91,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 219, 253,\n        253, 253, 253, 253, 253, 253, 253, 240, 102,  16,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n        219, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 218,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,  98, 253, 113,  78,  78, 138, 137, 127, 253, 253, 253,\n         96,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0]))"},"metadata":{}}],"execution_count":215},{"cell_type":"markdown","source":"# Model\n\nnow it's time to implement the math (yay...). ","metadata":{}},{"cell_type":"code","source":"# first, i'm going to initialize the parameters (w and b)\n\ndef init_params(): \n    W1 = np.random.rand(10, 784) * 0.01 # in the parantheses, i'm putting the dimensions of the array. \n    B1 = np.random.rand(10, 1)\n    W2 = np.random.rand(10, 10) * 0.01\n    B2 = np.random.rand(10, 1)\n\n    return W1, B1, W2, B2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:38.992160Z","iopub.execute_input":"2025-01-04T07:39:38.992446Z","iopub.status.idle":"2025-01-04T07:39:38.997486Z","shell.execute_reply.started":"2025-01-04T07:39:38.992424Z","shell.execute_reply":"2025-01-04T07:39:38.996302Z"}},"outputs":[],"execution_count":216},{"cell_type":"code","source":"# here i'm going to write the ReLU and softmax activation functions. \n\ndef relu(x): \n    # return 0 if x <= 0 else x (first one)\n    return np.maximum(0, x) # pretty much the same\n\ndef softmax(Z):\n    # Z -= np.max(Z, axis=0)  # Subtract max value for numerical stability\n    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)\n    return A","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:38.999210Z","iopub.execute_input":"2025-01-04T07:39:38.999605Z","iopub.status.idle":"2025-01-04T07:39:39.015042Z","shell.execute_reply.started":"2025-01-04T07:39:38.999543Z","shell.execute_reply":"2025-01-04T07:39:39.014054Z"}},"outputs":[],"execution_count":217},{"cell_type":"code","source":"# now, it's time to write forward prop! i'll see if i can do it without the video. \n\ndef forward_prop(W1, B1, W2, B2, X): \n    Z1 = np.dot(W1, X) + B1 # unactivated hidden layer\n\n    A1 = relu(Z1) # activated hidden layer\n    \n    Z2 = np.dot(W2, A1) + B2 # unactivated output layer\n\n    A2 = softmax(Z2)\n\n    return Z1, A1, Z1, A2\n# aaaaand that's forward prop! now backprop...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.015951Z","iopub.execute_input":"2025-01-04T07:39:39.016248Z","iopub.status.idle":"2025-01-04T07:39:39.030442Z","shell.execute_reply.started":"2025-01-04T07:39:39.016222Z","shell.execute_reply":"2025-01-04T07:39:39.029462Z"}},"outputs":[],"execution_count":218},{"cell_type":"code","source":"# one-hot encoding\n\ndef one_hot(x):\n    onehot_x = np.zeros((x.size, x.max() + 1))\n    onehot_x[np.arange(x.size), x] = 1\n    onehot_x = onehot_x.T\n    return onehot_x\n\n# i'll be honest, i have no idea what this means... copied ✌️","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.031285Z","iopub.execute_input":"2025-01-04T07:39:39.031553Z","iopub.status.idle":"2025-01-04T07:39:39.047627Z","shell.execute_reply.started":"2025-01-04T07:39:39.031529Z","shell.execute_reply":"2025-01-04T07:39:39.046614Z"}},"outputs":[],"execution_count":219},{"cell_type":"code","source":"# derivative of ReLU (copied idc idc)\n\ndef d_relu(x): \n    return x > 0\n# when booleans are converted to numbers, true = 1 and false = 0 (what he said idk idc) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.048812Z","iopub.execute_input":"2025-01-04T07:39:39.049125Z","iopub.status.idle":"2025-01-04T07:39:39.063674Z","shell.execute_reply.started":"2025-01-04T07:39:39.049066Z","shell.execute_reply":"2025-01-04T07:39:39.062607Z"}},"outputs":[],"execution_count":220},{"cell_type":"code","source":"# here i'll implement backprop. \n\ndef back_prop(W1, W2, A1, A2, Z1, Z2, X, Y): # Y will be one-hot encoded\n    y = one_hot(Y)\n    \n     # output layer\n    dZ2 = 2*(A2 - y)\n\n    dW2 = (1/m) * np.dot(dZ2, A1.T) # weights\n    dB2 = 1 / m * np.sum(dZ2, axis=1).reshape(-1, 1)\n\n    # hidden layer\n    dZ1 = np.dot(W2.T, dZ2) * d_relu(Z1)\n    \n    dW1 = (1/m) * np.dot(dZ1, X.T) # weights\n    dB1 = 1 / m * np.sum(dZ1, axis=1).reshape(-1, 1)\n\n    return dW1, dB1, dW2, dB2\n# yay... (i'm tired)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.066178Z","iopub.execute_input":"2025-01-04T07:39:39.066529Z","iopub.status.idle":"2025-01-04T07:39:39.078544Z","shell.execute_reply.started":"2025-01-04T07:39:39.066485Z","shell.execute_reply":"2025-01-04T07:39:39.077558Z"}},"outputs":[],"execution_count":221},{"cell_type":"code","source":"# separating the update of params bc he did\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2   \n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.079834Z","iopub.execute_input":"2025-01-04T07:39:39.080249Z","iopub.status.idle":"2025-01-04T07:39:39.093712Z","shell.execute_reply.started":"2025-01-04T07:39:39.080208Z","shell.execute_reply":"2025-01-04T07:39:39.092698Z"}},"outputs":[],"execution_count":222},{"cell_type":"code","source":"# accuracy + predictions (copied)\n\ndef get_pred(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(pred, Y):\n    print(pred, Y)\n    return np.sum(pred == Y) / Y.size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.094880Z","iopub.execute_input":"2025-01-04T07:39:39.095222Z","iopub.status.idle":"2025-01-04T07:39:39.110772Z","shell.execute_reply.started":"2025-01-04T07:39:39.095194Z","shell.execute_reply":"2025-01-04T07:39:39.109847Z"}},"outputs":[],"execution_count":223},{"cell_type":"code","source":"# now we can perform gradient descent \n\ndef gradient_descent(X, Y, iters, alpha):\n    W1, B1, W2, B2 = init_params()\n    for i in range(iters): \n        Z1, A1, Z2, A2 = forward_prop(W1, B1, W2, B2, X)   \n        dW1, dB1, dW2, dB2 = back_prop(W1, W2, A1, A2, Z1, Z2, X, Y)\n        W1, B1, W2, B2 = update_params(W1, B1, W2, B2, dW1, dB1, dW2, dB2, alpha)\n\n        if (i%10) == 0: \n            print(\"Iteration: \", i)\n            pred = get_pred(A2)\n            print(\"Accuracy: \", get_accuracy(pred, Y))\n    return W1, B1, W2, B2           ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.111832Z","iopub.execute_input":"2025-01-04T07:39:39.112234Z","iopub.status.idle":"2025-01-04T07:39:39.127919Z","shell.execute_reply.started":"2025-01-04T07:39:39.112198Z","shell.execute_reply":"2025-01-04T07:39:39.126916Z"}},"outputs":[],"execution_count":224},{"cell_type":"code","source":"# now time to apply it!\n\nW1, B1, W2, B2 = gradient_descent(X_train, Y_train, 1000, 1)\n \nprint(W1, B1, W2, B2) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T07:39:39.128871Z","iopub.execute_input":"2025-01-04T07:39:39.129214Z","iopub.status.idle":"2025-01-04T07:39:47.940367Z","shell.execute_reply.started":"2025-01-04T07:39:39.129186Z","shell.execute_reply":"2025-01-04T07:39:47.937921Z"}},"outputs":[{"name":"stdout","text":"Iteration:  0\n[1 1 1 ... 1 1 1] [5 2 8 ... 1 3 7]\nAccuracy:  0.11\nIteration:  10\n[3 3 3 ... 2 3 3] [5 2 8 ... 1 3 7]\nAccuracy:  0.10733333333333334\nIteration:  20\n[3 3 3 ... 3 3 3] [5 2 8 ... 1 3 7]\nAccuracy:  0.107\nIteration:  30\n[3 3 3 ... 3 3 3] [5 2 8 ... 1 3 7]\nAccuracy:  0.15733333333333333\nIteration:  40\n[0 0 3 ... 1 3 3] [5 2 8 ... 1 3 7]\nAccuracy:  0.25\nIteration:  50\n[0 0 3 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.3443333333333333\nIteration:  60\n[0 0 3 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.384\nIteration:  70\n[2 0 3 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.4736666666666667\nIteration:  80\n[2 2 3 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.5853333333333334\nIteration:  90\n[2 2 3 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.6516666666666666\nIteration:  100\n[2 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.698\nIteration:  110\n[2 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.729\nIteration:  120\n[2 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.7586666666666667\nIteration:  130\n[2 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.7796666666666666\nIteration:  140\n[2 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.7986666666666666\nIteration:  150\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.814\nIteration:  160\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8236666666666667\nIteration:  170\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8323333333333334\nIteration:  180\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8426666666666667\nIteration:  190\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.849\nIteration:  200\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8596666666666667\nIteration:  210\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8666666666666667\nIteration:  220\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8716666666666667\nIteration:  230\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8773333333333333\nIteration:  240\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.879\nIteration:  250\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8826666666666667\nIteration:  260\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8863333333333333\nIteration:  270\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8913333333333333\nIteration:  280\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8953333333333333\nIteration:  290\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.897\nIteration:  300\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.8993333333333333\nIteration:  310\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9026666666666666\nIteration:  320\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.906\nIteration:  330\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9076666666666666\nIteration:  340\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9096666666666666\nIteration:  350\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9116666666666666\nIteration:  360\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9133333333333333\nIteration:  370\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.915\nIteration:  380\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9166666666666666\nIteration:  390\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.919\nIteration:  400\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9203333333333333\nIteration:  410\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9223333333333333\nIteration:  420\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9233333333333333\nIteration:  430\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.924\nIteration:  440\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9253333333333333\nIteration:  450\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.927\nIteration:  460\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9273333333333333\nIteration:  470\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.928\nIteration:  480\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.928\nIteration:  490\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9293333333333333\nIteration:  500\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9296666666666666\nIteration:  510\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9306666666666666\nIteration:  520\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9313333333333333\nIteration:  530\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.933\nIteration:  540\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.934\nIteration:  550\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9356666666666666\nIteration:  560\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9363333333333334\nIteration:  570\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.937\nIteration:  580\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.937\nIteration:  590\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.938\nIteration:  600\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9386666666666666\nIteration:  610\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9393333333333334\nIteration:  620\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9406666666666667\nIteration:  630\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9423333333333334\nIteration:  640\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9423333333333334\nIteration:  650\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9436666666666667\nIteration:  660\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9456666666666667\nIteration:  670\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9473333333333334\nIteration:  680\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.948\nIteration:  690\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.948\nIteration:  700\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9483333333333334\nIteration:  710\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9493333333333334\nIteration:  720\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9493333333333334\nIteration:  730\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.95\nIteration:  740\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9513333333333334\nIteration:  750\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9516666666666667\nIteration:  760\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.952\nIteration:  770\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.953\nIteration:  780\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.954\nIteration:  790\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.954\nIteration:  800\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9546666666666667\nIteration:  810\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9553333333333334\nIteration:  820\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.955\nIteration:  830\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9556666666666667\nIteration:  840\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.956\nIteration:  850\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9563333333333334\nIteration:  860\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.957\nIteration:  870\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9576666666666667\nIteration:  880\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9576666666666667\nIteration:  890\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.958\nIteration:  900\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9583333333333334\nIteration:  910\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9586666666666667\nIteration:  920\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.959\nIteration:  930\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9596666666666667\nIteration:  940\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9603333333333334\nIteration:  950\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9606666666666667\nIteration:  960\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9613333333333334\nIteration:  970\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9616666666666667\nIteration:  980\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.9616666666666667\nIteration:  990\n[5 2 8 ... 1 3 7] [5 2 8 ... 1 3 7]\nAccuracy:  0.962\n[[3.03976368e-03 8.28806613e-03 5.90509729e-03 ... 6.43285909e-03\n  6.39556656e-03 2.76563135e-04]\n [9.11073858e-03 5.45306779e-03 5.10948755e-03 ... 8.05911591e-03\n  6.28949642e-04 7.55568447e-03]\n [5.43868214e-03 7.01308724e-03 3.34592766e-03 ... 8.10124866e-03\n  6.20954672e-03 7.07872338e-03]\n ...\n [7.02646365e-03 4.78667882e-06 4.34155078e-03 ... 4.45348737e-03\n  6.42815467e-03 6.63851718e-03]\n [8.24910691e-03 2.99729648e-03 5.92388865e-03 ... 5.72398830e-03\n  8.40317429e-04 6.80984469e-03]\n [7.86546332e-03 6.79025946e-03 4.35216312e-03 ... 9.83340529e-03\n  8.74284779e-03 7.58589450e-03]] [[0.55671414]\n [0.54371708]\n [0.76766359]\n [0.17027999]\n [0.57183139]\n [0.22460659]\n [0.69728874]\n [0.89200096]\n [0.60309132]\n [0.53350147]] [[ 0.75125154  0.52718702 -0.74833893  0.3887277  -0.16692469  0.81395649\n  -1.13107168 -0.427759   -0.63989547  0.62793414]\n [-0.18589008 -0.59299747 -0.66140085 -0.8668413   0.65073022 -0.87631358\n   1.33508021  0.20119508 -0.33960713 -0.61860684]\n [-0.1191983  -0.74996851 -0.52756632  1.54854599  0.85784416 -0.32699841\n  -0.37034368 -0.0119336   0.23223888 -0.46331819]\n [-0.82015317 -1.06621701  0.11238389  0.80219732 -0.11888164  0.4445243\n   0.6582122   0.5503523  -0.90098294  0.76869394]\n [-0.31030733  0.26378376  0.71514403 -0.45857308 -1.2265997  -0.14746972\n   0.07041995 -0.77013892  1.24907708 -0.0264352 ]\n [ 1.4014547   0.39932601  0.28356793 -0.34561782 -0.51246957  0.10622905\n   0.24907087 -0.0523178  -0.60085599  0.50562557]\n [-0.0499314   1.1580284  -0.50490673  0.72993009 -1.00311589 -1.00005238\n   0.12732668 -0.41630115  0.47651152 -0.57897362]\n [-0.54978048  0.17069504  0.52408487 -0.74287044  0.68416365 -0.2240623\n  -1.00454616  1.44803093 -0.03114621  0.62887207]\n [ 0.70783798 -0.12346809 -0.32316113 -0.31946425  0.49494612  0.46166626\n   0.29256413 -0.37698041  0.52981936 -0.12426251]\n [-0.7761443   0.06336967  1.17687754 -0.68723167  0.38170825  0.79772252\n  -0.18635206 -0.10164367  0.07294095 -0.66732523]] [[ 0.38175984]\n [ 1.35112925]\n [ 0.92519492]\n [ 0.33639096]\n [ 0.77266153]\n [ 0.77837656]\n [ 1.02603105]\n [ 0.45820736]\n [-0.2560895 ]\n [ 0.68669274]]\n","output_type":"stream"}],"execution_count":225},{"cell_type":"markdown","source":"after hours of debugging + not knowing what i'm doing, i achieved ~95% accuracy with 1000 iterations and a learning rate of 1 (??????). i have no idea what i did, but i'm tired. \n\ni can now say that i made a nn from scratch (with help.... don't tell anyone).\n\nok bye sleep time now. ","metadata":{}}]}